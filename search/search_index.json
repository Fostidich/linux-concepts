{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Advanced Operating Systems","text":""},{"location":"#topics-overview","title":"Topics overview","text":"<ul> <li>Multiplexing the CPU and address translation<ul> <li>Threads, processes and scheduling, concurrency</li> <li>Memory virtualisation</li> <li>System virtualisation</li> </ul> </li> <li>Interfacing and persistence<ul> <li>I/O, disks</li> <li>Booting</li> </ul> </li> <li>System-level programming<ul> <li>80% CPU Linux kernel programming (C)</li> <li>20% MCU OS programming (C++)</li> </ul> </li> </ul>"},{"location":"#advanced-concepts","title":"Advanced concepts","text":"<ul> <li>Scheduling policies (not only Linux's CFS)</li> <li>Virtual machines (hypervisors)</li> <li>Memory management (e.g., segmentation, buddy algorithm, Linux PFRA)</li> <li>Advanced kernel synchronization primitives (Linux's RCU and MCS)</li> <li>Modern C/C++ concurrency and design patterns</li> <li>Memory consistency models</li> <li>Booting and configuring a system (UEFI, UBoot, PCI, ACPI)</li> </ul>"},{"location":"boot/booting/","title":"Booting","text":"<p>General purpose PCs and embedded systems have their own booting mechanism (firmware interfaces) which are part of the firmware and which manage the main hardware initialization process, before calling bootloaders (like GRUB) which then launches the OS kernel passing the necessary settings. On GP systems the most common interface is UEFI, which is a modern alternative replacing the BIOS (legacy). On microcontrollers there are many simpler alternatives, with of the most common one being U-BOOT. Being just a firmware, UEFI alone is not enough to read and launch the OS, therefore a bootloader like GRUB is a necessary intermediary providing the essential flexibility and features (like reading complex file systems) for loading the kernel.</p>"},{"location":"boot/booting/#uefi","title":"UEFI","text":"<p>The Unified Extensible Firmware Interface (UEFI) offers a modular environment extensible with additional drivers. Upon powering the computer, UEFI is immediately executed, taking control initializing hardware and loading firmware settings from nvRAM (non-volatile RAM). A dedicated FAT32 EFI System Partition (ESP) holds the bootloader files for various OSs; for Linux this is called <code>elilo.efi</code>.</p>"},{"location":"boot/booting/#gpt","title":"GPT","text":"<p>BIOS was constrained to 32 bit addresses, hugely limiting indexable memory. UEFI uses instead 64 bits, addressing logical blocks in a GUID Partition Table (GPT). The GPT is a modern disk partitioning system, occupying sectors 1-33 at the beginning of the disk. Sector 0 (protected MBR) is reserved for backward compatibility with systems not recognizing GPT. The GPT lists many partition entries, each holding various data.</p> <ul> <li>The first 16 bytes of each entry tells the partition type (e.g. Linux FS).</li> <li>The second 16 stores the GUID (Globally Unique ID) of the partition.</li> <li>Location and size are described with the starting and ending LBAs.</li> <li>Various feature flags define attributes and permissions.</li> <li>The partition is assigned also a descriptive name.</li> </ul>"},{"location":"boot/booting/#u-boot","title":"U-BOOT","text":"<p>On embedded platform, a simpler BootROM is preferred over BIOS/UEFI. Those typically operate in two stages. The Fist Stage Bootloader (FSBL) is minimalistic software used for initial hardware setup. The second (SSBL) gets called when the FSBL completes and boots the OS. U-BOOT is one of many SSBLs. It is a mature, well-known bootloader and supports many CPU architectures (e.g. ARM, x86) and many peripheral drivers. It is also able to work on various file systems</p>"},{"location":"boot/platform-configuration/","title":"Platform configuration","text":""},{"location":"boot/platform-configuration/#acpi","title":"ACPI","text":"<p>When booting, the kernel must know the devices present on the machine. The firmware typically uses one of two standards to provide platform data: either the Advanced Configuration and Power Interface (ACPI, mainly used on general purpose systems) or Device Trees (DT, used in embedded devices).</p> <p>ACPI is a standardized method for initializing system components. Its abstraction permits a single kernel binary to run on different platforms. What the kernel receives from the ACPI is a set of tables containing AML (ACPI Machine Language) and registers. Tables with static data are obtained via the BIOS/UEFI firmware. ACPI registers (mapped via MMIO) can be used for control tasks, like checking the power status or set a sleep state. The ACPI driver in the kernel provides an interpreter able to execute AML code. Via the OSPM (Operating System-directed configuration and Power Management) module, the kernel can set, via ACPI, sleep states for the CPU, reducing consumption when idle. In user space, interactions with the ACPI can be done via a daemon like <code>acpid</code>.</p>"},{"location":"boot/platform-configuration/#namespace","title":"Namespace","text":"<p>One of the ACPI components is the Differentiated System Description Table (DSDT), a block of memory provided by the firmware containing AML. It contains a representation of all devices in the system (namespace) and the interfaces for managing their power states. For example, the state of a battery named <code>BAT0</code> can be obtained with <code>cat /proc/acpi/battery/BAT0/state</code>, which calls the <code>_BST</code> control method.</p>"},{"location":"boot/platform-configuration/#power-management","title":"Power management","text":"<p>High temperatures lower the reliability of hardware. Each component is labeled with a TDP (Thermal Design Power), describing the maximum heat generated under standard conditions. The main source of consumption are the voltage and the frequency. To ease power management, instead of controlling directly multiple inter-depending variables, devices are designed with predefined states.</p> <p>ACPI exposes five different layers of power states (G, S, C, P, D types).</p> <ul> <li>G-types group S-types and describe the system state in a high-level, behavioural way.</li> <li>S-types (system states) concretely define what G-types do.</li> <li>C-types refer to CPU states; via specific instruction, the clock speed can be reduced.</li> <li>P-types allow further control over the CPU performance state based on workload.</li> <li>D-types specifically refer to devices.</li> </ul> <p>G/S states are managed by the ACPI daemon, C/P/D by the OSPM. When the scheduler, controlling workload, requires a P state change, it communicates it to a OSPM module, which via the ACPI changes a given register (PCM, Performance Control Machine). In user space, events are forwarded via the <code>sysfs</code>, and the ACPI daemon (listening to it) transitions to the new state by editing <code>/sys/power/state</code>.</p>"},{"location":"boot/platform-configuration/#device-tree","title":"Device tree","text":"<p>Device trees are data structures that provide information about hardware topology where devices cannot be probed. They come in two forms: a textual, human-readable version (<code>.dts</code>) and a compile version that OSs can interpret (<code>.dtb</code>). DTs are written by developers, they are not auto-generated, therefore, you'd write the textual version for then compiling it into the blob.</p> <p>The DT contains hierarchical definitions for device nodes. Upon providing a DT specification name and version, the tree start from a root node, identified by <code>/</code>. Each node may have many properties, such as <code>compatible</code>, defining vendor and model names, used to match it with their specific driver. The <code>cpu</code> field is used to describe each CPU in the system. Many other nodes can be used to introduce other system components, like serial devices and flash memories.</p> <p>DTs also provide configuration capabilities for interrupt controllers and interrupt signal declarations, expressed as links between nodes. Interrupts are specified by providing an interrupt number and a set of optional flags.</p>"},{"location":"concurrency/kernel-space-concurrency/","title":"Kernel space concurrency","text":"<p>Kernel concurrency is crucial for modules to function correctly in multithreaded environments. Unlike user\u2011space programs, the kernel handles interrupt processing, spinning waits, and re-entrant code. Concurrency in kernels can have three sources: interrupts, multi-core systems or preemption.</p> <p>Note than in preemptive systems, after a interrupt finishes execution, the kernel may switch to a process in user space different from the last executing one.</p> <p>Linux kernel is optionally non-preemptive. In those cases a task in the kernel explicitly has to call <code>schedule</code> (when knowing it is safe to reschedule). Therefore, a planned context switch can happen if the process puts itself knowingly in a wait queue.</p>"},{"location":"concurrency/kernel-space-concurrency/#atomic-operations","title":"Atomic operations","text":"<p>Atomic contexts are sections of code where a preemption is not safe.</p> <p>Typical atomic contexts are the following.</p> <ul> <li>The kernel is within an interrupt handler</li> <li>The kernel is holding a spinning lock<ul> <li>This could lead to deadlocks or inconsistent state due to half\u2011completed critical sections</li> </ul> </li> <li>The kernel is modifying a CPU structures</li> <li>The kernel state cannot be restored completely with a context switch<ul> <li>For example when executing in a FPU, as micro-architectural states cannot be simply copied and restored by the kernel.</li> </ul> </li> </ul> <p>In kernel code, <code>preempt_disable</code> and <code>preempt_enable</code> can be used to define an atomic context.</p> <p>In real-time patched kernels, threads that must avoid interruption can set their priority to a value higher than fifty. Moreover, the interrupt handler is designed to perform minimal tasks, improving response time.</p>"},{"location":"concurrency/lock-free-synchronization/","title":"Lock-free synchronization","text":"<p>Lock-free algorithms allow concurrent accesses to shared data without using locks, therefore improving overall performance.</p>"},{"location":"concurrency/lock-free-synchronization/#per-cpu-variables","title":"Per-CPU variables","text":"<p>One way to reduce shared data is to use per-CPU variables. A per-CPU variable is an array indexed by the CPU number, so that each CPU can access and modify only its own elements. During variable manipulation, preemption should be disabled to avoid changing CPUs mid-work.</p> <pre><code>// `counter` is an array of int, one for each CPU.\nDEFINE_PER_CPU(int, counter);\n\nvoid increase_counter(void) {\n    // Get the address of the correct CPU's element.\n    // `get_cpu_var` also disables preemption.\n    int *__counter = &amp;get_cpu_var(counter);\n    // Increase counter.\n    // This operation is directly reflected on the actual structure, as it is\n    // achieved via reference.\n    (*__counter)++\n    // `put_cpu_var` re-enables preemption.\n    put_cpu_var(counter);\n}\n</code></pre>"},{"location":"concurrency/lock-free-synchronization/#atomic-operations","title":"Atomic operations","text":"<p>There exists Linux APIs that enforce the use of atomic read\u2011modify\u2011write instructions available from the ISA. They are based on the <code>atomic_t</code>/<code>atomic64_t</code> types and architecture's CAS instructions; they guarantee that operations are not interruptible.</p> <pre><code>atomic64_t v = ATOMIC_INIT(0);\natomic64_add(2, &amp;v);\natomic64_long_add_unless(&amp;v, 2, 10)\n</code></pre> <p>Using CAS in concurrent data structures may be challenging or insufficient, as they are based on a static value of variables at a given time, and do not have a wide enough view of its history or of other variables to ensure more complex operations complete securely.</p>"},{"location":"concurrency/lock-free-synchronization/#read-copy-update","title":"Read copy update","text":"<p>RCU is mainly used on read-mostly data structures (like lists). It allows for fast concurrent read accesses without enforcing the use of locks, but with the downside that readers must be carefully coded to tolerate slightly inconsistent states.</p> <p>Readers use <code>rcu_read_lock</code> and <code>rcu_read_unlock</code> to mark critical sections, disabling preemption. Reads are always available, they can execute concurrently and are not subject to waiting queues.</p> <p>Regarding writers, they typically still have to acquire spinlocks to make updates, since only one writer can safely make changes. For additions or updates, writers prepare new nodes or copies of data separately and then atomically update pointers. Deletions follow a similar approach but freeing the memory is deferred until the end of a grace period. A grace period ends when all CPUs have reported quiescent states (points where no reader's critical section is running), signaled by context switches. <code>rcu_read_unlock</code> doesn't directly inform the writer; it just helps the system track grace periods. Writers learn that a grace period has ended when the RCU subsystem confirms it. <code>synchronize_rcu</code> is used to block the writer until all pre-existing RCU readers' critical sections complete, after which the writer knows it is safe to reclaim memory and thus finalize updates.</p> <pre><code>void reader() {\n    // Disable preemption and inform the RCU subsystem of a new reader.\n    rcu_read_lock();\n    // Here, reads can be achieved as usual, safely.\n    // Re-enable preemption.\n    rcu_read_unlock();\n    // The end of the grace period for this CPU will be known by the RCU\n    // subsystem upon the next preemption.\n}\n\nvoid writer() {\n    // A lock over the data structure is still needed by writers.\n    spin_lock(&amp;list_lock);\n    // Here, operations over the list can be achieved by atomically updating\n    // pointers.\n    // Upon deletions, frees must be deferred for a grace period.\n    // Let's say we here removed node `node` from the list.\n    // We do this by calling this list-specific function.\n    // `node` is removed, but not freed.\n    list_del_rcu(&amp;node-&gt;list);\n    // Free the lock.\n    spin_unlock(&amp;list_lock);\n    // Wait for the grace period to conclude, meaning all CPUs have passed\n    // through at least one quiescent state since `synchronize_rcu` started.\n    // All CPUs are waited, even if they have not read from the data structure\n    // this writer updated.\n    synchronize_rcu();\n    // Now it is possible to free the node.\n    kfree(node);\n}\n</code></pre>"},{"location":"concurrency/locking-synchronization/","title":"Locking synchronization","text":""},{"location":"concurrency/locking-synchronization/#basic-lock-types","title":"Basic lock types","text":"<ul> <li>Sleeping locks are employed when a task is put to sleep if a lock isn't     available, like during file system or network operations<ul> <li>Mutexes provide mutual exclusion, where only a single task     can hold the lock at a time</li> <li>Semaphores allow for a set number of tasks to own the lock, via an     internal counter</li> </ul> </li> <li>Spinlocks are utilized in atomic and interrupt contexts to     protect shared kernel data structures, without sleeping but spin-waiting<ul> <li>Read\u2011write locks allow readers to hold the lock     simultaneously, but writers to get exclusive access</li> <li>Seqlocks are used when there are many readers but few     writers, allowing readers to avoid locking, but checking that data     wasn't updated during the read</li> </ul> </li> </ul>"},{"location":"concurrency/locking-synchronization/#sleeping-locks","title":"Sleeping locks","text":""},{"location":"concurrency/locking-synchronization/#mutexes","title":"Mutexes","text":"<pre><code>static DECLARE_MUTEX(lock);\n\n// Calling `down_interruptible` puts the process in waiting status on the\n// lock's queue.\n// The function returns when the process is awaken.\nif (down_interruptible(&amp;lock)) {\n    // Lock not acquired: if a signal is received while in waiting queue, the\n    // process is kicked from the queue, as the process is awaken early for\n    // executing the signal handler.\n} else {\n    // Lock acquired: critical region is executing.\n    // Lock is released at the end.\n    up(&amp;lock);\n}\n</code></pre>"},{"location":"concurrency/locking-synchronization/#spinlocks","title":"Spinlocks","text":""},{"location":"concurrency/locking-synchronization/#basic-spinlock","title":"Basic spinlock","text":"<pre><code>DEFINE_SPINLOCK(lock);\n\nspin_lock(&amp;lock);\n// Lock acquired: critical region is executing.\n// Lock is released at the end.\nspin_unlock(&amp;mr_lock);\n</code></pre> <p>All spinlocks have a <code>irq</code> variant which prevents the execution of local interrupts, which in some cases can cause deadlocks.</p> <pre><code>DEFINE_SPINLOCK(lock);\n\nunsigned long flags;\n\nspin_lock_irqsave(&amp;lock, flags);\n// Lock acquired: critical region is executing.\n// No interrupt can execute here.\n// Lock is released at the end.\nspin_unlock_irqrestore(&amp;mr_lock, flags);\n</code></pre> <p>Spinlocks are often implemented exploiting machine\u2011provided atomic instructions such as the atomic compare\u2011and\u2011swap (CAS, <code>_atomic_compare_xchg</code>). This guarantees that only one thread can update the variable at a time in SMP systems.</p>"},{"location":"concurrency/locking-synchronization/#rwlocks","title":"RWlocks","text":"<pre><code>DEFINE_RWLOCK(lock);\n\nread_lock(&amp;lock);\n// Lock acquired: can read.\n// Many tasks can own the read lock simultaneously.\nread_unlock(&amp;lock);\n\nwrite_lock(&amp;lock);\n// Lock acquired: can read and write.\n// Only one task can own the lock when writing.\nwrite_unlock(&amp;lock);\n</code></pre> <p>When having many readers, writers may be starved. Seqlocks try to fix this issue.</p>"},{"location":"concurrency/locking-synchronization/#seqlocks","title":"Seqlocks","text":"<pre><code>DEFINE_SEQLOCK(lock);\n\nwrite_seqlock(&amp;lock);\n// Lock acquired: can read and write.\n// Only one task can own the lock when writing.\n// Sequence counter is increased both when locking and unlocking.\nwrite_sequnlock(&amp;lock);\n\ndo {\n    // Store current sequence number.\n    // It will always be even, as the function will spin until it becomes such.\n    seq = read_seqbegin(&amp;lock);\n    // Read data.\n    // Loop if sequence number has changed.\n} while (read_seqretry(&amp;lock, seq));\n</code></pre>"},{"location":"concurrency/locking-synchronization/#deadlocks","title":"Deadlocks","text":"<p>When configured, the kernel can run with a runtime mechanism (called <code>lockdep</code>) for checking deadlocks, by analyzing locking sequences. This usually is a debugging option, as it adds a big overhead. It keeps track of locking sequences, comparing them, and checks spinlocks acquired during interrupts or interrupts-enabled contexts.</p> <p>Deadlocks can appear during interrupt handlers. If the executing process takes a spinlock over a resource, and an interrupt takes over before it is released, a deadlock can appear if the handler waits for the same resource. Moreover, in multi-processors architecture, deadlocks can also be caused by interrupts received on other CPUs. This involves two resources, and happens when the first CPU is locking A and requesting B, while the second CPU is locking B and an interrupt requests A. These are the cases in which to use <code>irq</code> variants.</p>"},{"location":"concurrency/locking-synchronization/#cache-aware-spinlocks","title":"Cache aware spinlocks","text":""},{"location":"concurrency/locking-synchronization/#the-ping-pong-problem","title":"The ping pong problem","text":"<p>The ping pong problem occurs when multiple CPUs want to acquire a spinlocks over the same resource, therefore they spin over the same lock. Doing so, they repeatedly access and update the same lock memory location, continuously invalidating the cache line for that variable in all other CPUs. While spinning, when CPUs find the lock variable invalid in the cache they need to reload it, which is slow. This happens in every CPU upon each write of the variable, happening at minimum twice per previous lock holder (when acquiring and when releasing). The ping pong problem is not a deadlock problem, but a performance degradation problem.</p>"},{"location":"concurrency/locking-synchronization/#mcs-lock","title":"MCS lock","text":"<p>The MCS (Mellor-Crummey and Scott) locks solve this problem by using a queue-based locking mechanism. Instead of all CPUs spinning on a single shared variable, each CPU spins on a separate local variable. When a CPU fails to acquire the lock, it enqueues itself in a linked list and spins only on its own node's lock. It also asks the previous task in line (which could/will be lock owner) to hand off the lock when releasing it, updating the local lock and making the next in queue exit the spinning.</p>"},{"location":"concurrency/locking-synchronization/#qspinlocks","title":"Qspinlocks","text":"<p>Qspinlocks are MCS locks optimized for specific hardware and performance goals. In Linux qspinlocks, the first spinning CPU waiting in line for the lock will not enqueue itself, but will keep spinning on the same lock of the holder, as it won't cause ping pong effects. This is because the first write on the lock will be the one freeing it, and a reload from the next CPU will be required anyway.</p>"},{"location":"concurrency/memory-consistency-models/","title":"Memory consistency models","text":"<p>Modern CPUs can incorporate instruction reordering in their design, for performance reasons. This means that operations may be executed in a order different from the one appearing in the program, but it is ensured that the execution logic is left unchanged.</p> <p>However, on SMP systems with CPUs executing concurrently, these operations may be perceived differently across threads. For instance, the reordering of writes in a thread can produce a theoretically impossible trace in another CPU.</p> <pre><code>// Initial state\nx = 0;\ndone = 0;\n\n// Thread 1\nx = 1;\ndone = 1;\n\n// Thread 2\nwhile(done == 0);\nprint(x);\n</code></pre> <p>Here, thread 2 should only print 1, but if thread 1 swaps its writes, it is possible to see 0 printed.</p>"},{"location":"concurrency/memory-consistency-models/#sequential-consistency","title":"Sequential consistency","text":"<p>Sequential consistency is a property that programs must satisfy when compiled. It enforces that instruction ordering follows the \"happens\u2011before\" relation, meaning that the order of instructions in a program is the same as the order in which they are visible in shared memory.</p> <p>Execution between threads now is always consistent with how programs are defined, but performance can be reduced.</p>"},{"location":"concurrency/memory-consistency-models/#tso-model","title":"TSO model","text":"<p>In a TSO model (Total Store Order), which is the one x86 use, each CPU has its own buffer in which it temporarily stores writes, allowing reordering in instructions but showing shared memory changes as the program's order would. However, by enqueuing writes, consistent (instant) visibility across threads may be broken, and thus the locking algorithms depending on it.</p>"},{"location":"concurrency/memory-consistency-models/#pso-model","title":"PSO model","text":"<p>ARM's Partial Store Order model has an even more relaxed architecture than TSO, as each CPU has its own shared memory copy, with writes propagating between processors independently, and without enforcing writes ordering. Such flexibility means higher concurrency but can complicate reasoning about memory consistency.</p>"},{"location":"concurrency/memory-consistency-models/#data-races","title":"Data races","text":"<p>A data race between threads can be produced if executing instructions comprise at least one write operation. When writes order isn't guaranteed, potential misalignment between intended and actual execution can appear in a data race context.</p> <p>CPUs fulfilling the DRF (Data Race Free) synchronization model provide operations to coordinate reads and writes.</p>"},{"location":"concurrency/memory-consistency-models/#fences","title":"Fences","text":"<p>Fences (barriers) ensure that memory operations can be reordered within, but not across them. DRF programs set those barriers so that the program ensures \"happens-before\" relations. Two threads accessing the same memory location will either read or have synchronization operations that ensure one's memory access occurs before the other's.</p>"},{"location":"concurrency/memory-consistency-models/#store-release-and-load-acquire","title":"Store release and load acquire","text":"<p>The <code>smp_store_release</code> and <code>smp_load_acquire</code> functions are used to enhance basic load and store instructions. SR ensures that before writing to memory, all the thread's previous operations are pushed for visibility in other threads. LA instead observes SRs, waiting until the read operation can see the latest value.</p>"},{"location":"concurrency/memory-consistency-models/#read-and-write-once","title":"Read and write once","text":"<p><code>READ_ONCE</code> and <code>WRITE_ONCE</code> are primitives in C which can be used to read and write variables while disallowing the compiler to reorder them, omit reads if values appear unchanged, or insert excessive reads due to register spills.</p>"},{"location":"drivers/cpu-device-communication/","title":"CPU device communication","text":""},{"location":"drivers/cpu-device-communication/#port-and-memory-bus","title":"Port and memory bus","text":"<p>Connection between the CPU and devices can be achieved via channels named \"bus\". Communication can happen over two buses: the port and the memory bus. Along with data transfers, you would also write commands into these registers.</p> <p>In port-based IO (legacy), device registers are accessed using special CPU instructions. x86 systems have a UART (Universal Asynchronous Receiver\u2011Transmitter) serial hardware interface for mapping device registers to the device port numbers. Each device uses one port for each of its register, therefore one device may use several ports: they act as their own address space.</p> <p>In memory-mapped IO devices use registers mapped to reserved memory addresses. MMIO is more convenient as it lets the CPU use classic loads and stores as if it was writing to RAM memory cells. The hardware is then responsible to deliver this data directly to the device instead.</p> <p>Port-based IO and memory-mapped IO both access device registers, but the first uses a dedicated address space with ad-hoc instructions.</p>"},{"location":"drivers/cpu-device-communication/#device-access-request","title":"Device access request","text":"<p>Port-based IO requests access to a port range, mapped using <code>ioport_map</code>. Those ports/registers are accessed via instructions such as <code>inb</code>, <code>outb</code>, <code>inw</code>, <code>outw</code>, <code>inl</code>, <code>outl</code>, based on their size. Memory-mapped IO requests a memory region instead, mapping it via <code>io_remap</code>. Linux also provides generic APIs like <code>iowrite</code>, which abstracts the bus used, translating the same call either on PBIO or MMIO.</p>"},{"location":"drivers/cpu-device-communication/#notifications","title":"Notifications","text":"<p>When a device completes its task, it must notify the CPU.</p> <p>The simplest way is to use polling, where the CPU continuously checks onto device registers for events (like spinlocks). This is efficient for fast devices but very wasteful for slow ones.</p> <p>On slow devices a better alternatives is to use interrupts. When waiting for the device to respond, the waiting process is put to sleep and context is switched; when the device completes, it raises an hardware interrupt which causes the OS to stop the current execution on the CPU and start the registered interrupt service routine (ISR). An ISR is a custom function written by the device driver dynamically registered within the kernel.</p> <p>An alternative solution is using DMAs, which let devices write data directly to mapped RAM addresses, bypassing the CPU, leaving it free of executing something else. Typically, after data has been written asynchronously, the device might call an interrupt to notify the CPU.</p>"},{"location":"drivers/device-management/","title":"Device management","text":""},{"location":"drivers/device-management/#device-categories","title":"Device categories","text":"<p>Linux supports three kinds of devices: character devices, block devices and network devices. Each is accessed in a unique way.</p> <p>Character devices are peripherals like keyboards and monitors, and they can be accessed as normal files, directly, without buffering. Block devices, like hard disks, can be accessed only in multiples of block size. Network devices use specific interfaces and subsystems.</p> <p>Linux tries to hide hardware details as much as possible, by integrating devices' drivers into the file system as special files, usually found in the <code>/dev</code> folder. Each driver is assigned with a major device identification number, with each device using it assigned with a minor device number.</p>"},{"location":"drivers/device-management/#character-devices","title":"Character devices","text":"<p>When registering a character device, the diver must implement the <code>file_operations</code> interface, defining functions to manage operations such as <code>read</code>, <code>write</code> and <code>*ioctl</code>.</p> <pre><code>struct file_operations {\n    ssize_t (*read) (struct file *, char __user *, size_t, loff_t *);\n    ssize_t (*write) (struct file *, const char __user *, size_t, loff_t *);\n    int (*open) (struct inode *, struct file *);\n    /* many other operations */\n}\n</code></pre>"},{"location":"drivers/device-management/#block-devices","title":"Block devices","text":"<p>Data accesses on block devices are achieved using a layered approach. First, the VFS (Virtual File System) receives requests for file accesses and translates them to page operations. If the page is in the page cache, data is sent/returned directly. Otherwise, the VFS uses the mapping layer to determine which device blocks hold that page. Requests are then forwarded to the block IO layer (<code>bio</code>), which aggregates accesses before passing them to the driver. Requests need to be enqueued, therefore the device driver must implement a <code>request_queue</code>. When accessing the device in <code>/dev</code>, a <code>gendisk</code> object can be reached which holds, among other disk information, a pointer to the <code>request_queue</code>.</p>"},{"location":"drivers/device-management/#requests-compression","title":"Requests compression","text":"<p>When accessing a non-cached page, the mapping layer first identifies the segments (contiguous sectors on disk) composing the page. Each segment is translated to a <code>bio_vec</code>, which is its kernel representation comprehending other data like locations and offsets.</p> <p>Then, all the <code>bio_vec</code> of the page are collected and aggregated where possible, creating new structures simply called <code>bio</code>, describing a contiguous area in memory. Each <code>bio</code> is then converted to an actual request sent to the block layer.</p> <p>The block layer tries to further optimize <code>bio</code> structs, by merging several physically adjacent requests, coming from different page accesses, together waiting in the staging queue. Staged requests are delayed by a factor depending on system load. This first queue is called the \"software\" queue.</p> <p>Requests get then forwarded to the \"hardware\" queue, which is directly read by the driver sending to the device. Fast storage devices can support multiple hardware queues, which permit concurrent execution of multiple requests.</p>"},{"location":"drivers/device-management/#scheduling","title":"Scheduling","text":"<p>How <code>bio</code> requests must be submitted depends on a IO scheduler algorithm, whose purpose is to improve performance and ensure fair bandwidth use, by minimizing delays, sorting requests or reflecting priorities. The following are the main IO schedulers proposed by Linux.</p> <ul> <li>Noop: ideal for SSDs, as they don't have slow mechanical parts.</li> <li>Budget Fair Queueing: bandwidth is divided equally.</li> <li>Kyber: reads, which are faster, are prioritized over writes, depending on     execution time.</li> <li>MQ-Deadline: reads are prioritized based on operation deadlines.</li> </ul>"},{"location":"drivers/device-management/#device-model","title":"Device model","text":"<p>Since the Linux kernel runs on many different architectures, it aims at maximizing code reusability, via various layers of abstraction and device representations. The high-level device model has many purposes, such as creating a device hierarchy, binding drivers to devices, power management, hot-plugging or device events forwarding.</p> <p>A driver operates on three main components: <code>struct device</code>, <code>struct device_driver</code>, <code>struct bus_type</code>. These structs are often extended with custom data to better represent underlying hardware details.</p>"},{"location":"drivers/device-management/#matching-flow","title":"Matching flow","text":"<p>Linux sees devices typically using only one of two firmware interfaces, depending on architectural choices: ACPI or device tree. Firmware sees hardware and builds a structure representing the device topology; this structure is either an ACPI table or a device tree blob, which the kernel can lookup. ACPI supports sending events to the kernel, whereas DT exploits interrupts.</p> <p>The fist element setup is the bus. A bus receives events of additions and removals of devices and manages their linkage with the corresponding driver. Drivers are registered to their bus via <code>register_driver</code>. When a device is connected, its driver sends a <code>probe</code> which initializes it. Devices are discovered by the system, which notifies its bus; devices can appear at runtime.</p>"},{"location":"drivers/device-management/#kobjects","title":"Kobjects","text":"<p>Kobjects are the kernel representation of objects like bus, divers and devices. They are exposed to user space via the <code>sysfs</code>, which maps objects to directories and their attributes to the files they contain. Kobjects are hierarchically organized in ksets (folders), mimicking hardware topology. Once an object is created (e.g. in <code>/sys/usb/devices</code>) the user device manager (<code>udev</code>) detects it and creates a device node in <code>/dev</code>. <code>udev</code> is a daemon that manages device nodes based on kernel events from <code>sysfs</code>.</p>"},{"location":"drivers/device-management/#bus-framework","title":"Bus framework","text":"<p>To simplify interactions with complex bus systems, the Linux kernel uses an abstraction layer (bus frameworks) to represent different hardware communication channels.</p> <p>Examples are the Platform BF, which is used to retrieve fixed hardware blocks or chips at boot time, USB, which supports hot-plugging by periodically polling the hubs status, or PCI, which adopts interrupts for notifying self-enumeration. Platform chips can be read in the ACPI/DT, USB builds a port-based tree of devices, PCI assigns bus numbers to nodes organized in a matrix/graph.</p>"},{"location":"drivers/device-management/#pcie","title":"PCIe","text":"<p>PCI Express is a point-to-point, high-speed bus standard, which may link CPU and GPU, SSDs and network interfaces. The PCI topology comprises three types of components, organized in a tree/hierarchy: the root complex, endpoints and bridges. These nodes are connected with buses (mainly PCIe). The root complex is the root of this tree, and is the interface used by the host (CPU and memory) to access devices. PCIe leaf devices are called endpoints. Bridges are internal tree nodes connecting the root complex to endpoints. They may also connect to some non PCIe device, acting as translators. Each PCI node is assigned with an ID in the form of <code>bus:device.function</code>: bus and device are just the corresponding hardware IDs; function is used to differentiate logical partitions of the device.</p> <p>The identification of devices in the kernel is called enumeration. At boot time, the firmware reserves space (typically via MMIO) for storing a configuration structure, maintaining devices data like device ID, vendor ID and BARs (Base Address Registers). Linux retrieves PCI configurations and setups the necessary drivers. Upon detection, the driver sends the <code>probe</code> function, reading configurations end establishing the connection.</p> <pre><code>int probe(struct pci_dev *pdev, const struct pci_device_id *id) {\n    int ret;\n    // BAR number (from driver datasheet knowledge).\n    int b = 0;\n    // Enable PCI device power and memory IO.\n    pci_enable_device(pdev);\n    // Request exclusive access to BAR b memory region.\n    ret = pci_request_region(pdev, b, \"pci-driver\");\n    // Get BAR start address and length from PCI config space.\n    resource_size_t mmio_start = pci_resource_start(pdev, b);\n    resource_size_t mmio_len = pci_resource_len(pdev, b);\n    // Map physical MMIO into kernel virtual address space.\n    void __iomem *hwmem = ioremap(mmio_start, mmio_len);\n    // Test MMIO write using a magic value to verify hardware access.\n    iowrite32(0xbaddcafe, hwmem);\n    // Allocate only 1 IRQ vector.\n    ret = pci_alloc_irq_vectors(pdev, 1, 1, PCI_IRQ_ALL_TYPES);\n    // Get the allocated IRQ vector number.\n    int irq = pci_irq_vector(pdev, 0);\n    // Register IRQ handler for device interrupts.\n    ret = request_irq(irq, irq_handler, 0, \"pci-driver\", pdev);\n    // Here you would save state (`hwmem`, `irq`) in some `pci_set_drvdata`.\n    return 0;\n}\n</code></pre>"},{"location":"drivers/interrupt-management/","title":"Interrupt management","text":""},{"location":"drivers/interrupt-management/#device-irq-flow","title":"Device IRQ flow","text":"<p>Interrupt are signals sent from devices initially to a IOAPIC (IO Advances Programmable Interrupt Controller) pin. They then forward the interrupt to a CPU's LAPIC (Local APIC), which enable Linux to run the corresponding IRQ (interrupt request) handlers. IOAPICs are chips available system-wide, and they can be used by all cores; each core has a single LAPIC component.</p> <p>Each IOAPIC has many pins, assignable to multiple devices; a pin belongs to a single IOAPIC. A device generally connects to a single IOAPIC pin. IOAPICs receive hardware interrupts from devices, and they in turn forward them to the correct LAPIC. When registering an interrupt in a IOAPIC, the pin is assigned with an interrupt ID (8-bit vector); it is also mapped with the LAPIC ID to which it must be sent. Pins are indexed via their <code>hwirq</code> number, but get mapped to a new vector as this way they become unique among all IOAPICs and they can represent other information too, such as priority. After the LAPIC receives the vector from a IOAPIC, Linux eventually runs the function (<code>irq_flow_handler_t</code>) mapped to the interrupt, which can chain many IRQ handler functions. These IRQ handler functions are also provided by the driver at setup-time.</p>"},{"location":"drivers/interrupt-management/#kernel-irq-flow","title":"Kernel IRQ flow","text":"<ol> <li>When an IRQ arrives, the CPU stops listening for other local lower-priority     interrupts, and context switches.</li> <li><code>__do_irq</code>/<code>__handle_irq</code>, the main interrupt dispatcher (entry point) of     the kernel, gets called after finding IRQ associated data from the 8-bit     vector.</li> <li>The <code>irq_flow_handler_t</code> function associated with the IRQ gets called.     It abstracts all specific details/protocols for managing the interrupt.</li> <li>A call to <code>irq_mask_ack</code> tells interrupt chips (e.g. APICs) that the IRQ     has been received, and to avoid further deliveries.</li> <li>Finally, the flow handler calls all the ISR actions registered in the driver     for that interrupt.</li> <li>When finished, <code>irq_unmask</code> tells the chip to resume IRQ deliveries.</li> </ol>"},{"location":"drivers/interrupt-management/#handler-registration","title":"Handler registration","text":"<p>To register an interrupt handler, first define the function with a set prototype (<code>irq_handler_t</code>). Non-critical work should be deferred as handlers should be fast and responsive. Also, they should not rely on process-specific data, as they are not executed in process contexts. The pointer to the function must then be passed to <code>request_irq</code>. More that a single function can be associated with a <code>irq</code> line number.</p> <pre><code>int request_irq(unsigned int irq, irq_handler_t handler,\n                unsigned long flags, const char *name, void *dev)\n</code></pre>"},{"location":"drivers/interrupt-management/#deferring-work","title":"Deferring work","text":"<p>Non-critical work is deferred with the soft IRQs APIs. Interrupt handlers can register functions to be executed later. This is useful as the CPU leaves more time available for receiving interrupts. Pending work gets executed when calling <code>do_softirq</code>, happening either after returning from hard IRQs, or from the <code>ksoftirqd</code> kernel threads, run when too much work piles up.</p> <p>The \"top half\" of the interrupt runs minimal work in non-interruptible context, deferring the rest later. The \"bottom half\" finalizes the work at reconciliation points. Soft IRQs can be of a few types.</p>"},{"location":"drivers/interrupt-management/#tasklets","title":"Tasklets","text":"<p>Tasklets are functions which the kernel ensures run once and securely on a single CPU, without sleeping. From the same IRQ, different tasklets with distinct handlers can be registered, but the same handler cannot be schedules twice, avoiding redundant runs if interrupts fire rapidly.</p> <p>Tasklets are enqueued in a list of <code>tasklet_struct</code>; during reconciliation points, the kernel checks this list for scheduled tasks. The struct is registered with <code>tasklet_schedule</code>.</p> <pre><code>DECLARE_TASKLET(tasklet, handler);\ntasklet_schedule(&amp;tasklet);\n</code></pre>"},{"location":"drivers/interrupt-management/#work-queues","title":"Work queues","text":"<p>If deferred actions must block (use locks, perform IO), they can be appended to a work queue, whose work gets executed later, in process context, by worker kernel threads (named \"events/0\", \"events/1\"...).</p> <pre><code>DECLARE_WORK(work, handler);\nschedule_work(&amp;work);\n</code></pre>"},{"location":"drivers/interrupt-management/#timers","title":"Timers","text":"<p>Precise sub-milliseconds timing can be achieved via the <code>hrtimer</code> subsystem.</p> <pre><code>struct hrtimer timer;\nhrtimer_setup(&amp;timer, handler, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\nhrtimer_start(&amp;timer, ms_to_ktime(100), HRTIMER_MODE_REL);\n</code></pre>"},{"location":"kernel/overview/","title":"Overview","text":"<p>An operating system is a program that acts as an intermediate layer between applications and the hardware.</p>"},{"location":"kernel/overview/#goals","title":"Goals","text":"<p>In general, the main goals of a OS are resource management, isolation and protection, portability and extendability.</p> <p>The OS manages resources between multiple applications by creating programs as if they were assigned their own set of resources (CPU, memory, disk), while ensuring fair utilization. Reliability and security is ensured by regulating access rights to resources, such as memory or files. OSs also hides the complexity of hardware management, adopting a facade pattern, often allowing the same applications to run on systems equipped with different physical resources. Low-level interfaces are uniformly developed so that high-level layers can be reused: this is achieved by utilizing device drivers which hide the undergoing complexity of the peripherals, introducing a bridge pattern.</p>"},{"location":"kernel/overview/#techniques","title":"Techniques","text":""},{"location":"kernel/overview/#resource-management","title":"Resource management","text":"<p>Latency can be reduced by increasing CPU utilization. Processes are interleaved by applying context switches either after a regular time quantum or when one gets in a waiting status. This can happen at the time of interrupts (coming from e.g. timers, the disk or the network), exceptions and system calls. The context of the current process is written in the PCB while the PCB of the next is put into the machine state.</p> <p>The Process Control Block (PCB) is a kernel structure containing process data, it contains:</p> <ul> <li>the Process Identifier (PID);</li> <li>the architectural state including the values of registers and counters in the CPU;</li> <li>the memory mappings linking the virtual address space to physical memory;</li> <li>open files, to ensure that the process can resume interactions after pausing;</li> <li>credentials, like user and group IDs, used to determine access levels for resources;</li> <li>signal handlers which are called to manage asynchronous events like interrupts;</li> <li>the controlling terminal, if any, along priority details;</li> <li>statistics queried to monitor performance and usage.</li> </ul> <p>Processes are switched based on the scheduling criteria adopted by the OS, which balances many indices. General Purpose OSs (GPOS) weights more fairness and throughput, whilst Real Time OSs (RTOS) deadlines, priority and efficiency.</p> <p>Linux OSs usually implement one of the following policies.</p> <ul> <li>FIFO (RT): a task runs until it finishes or a higher-priority task needs the CPU (no time slice).</li> <li>Round Robin (RT): each task gets a time slice; when its turn ends, the next equal-priority task runs.</li> <li>Completely Fair Scheduler (GP): Linux measures how much CPU time each task should get based on various dynamic factors.</li> <li>EEVDF (GP): the scheduler computes a virtual deadline, and runs the task having the soonest (improved CFS).</li> <li>Earliest Deadline First (RT): the task whose deadline is closest runs first.</li> </ul>"},{"location":"kernel/overview/#isolation-and-protection","title":"Isolation and protection","text":"<p>The memory a process can use is a Virtual Address Space (VAS). This region is isolated from other processes, but some locations may be shared. Virtual and physical space usages do not coincide: virtually, the process has an abundant contiguous chunk of memory, but physically resources are chopped up in pages and scattered all around the memory, even with some possibly found in the swap area on the disk.</p>"},{"location":"kernel/overview/#portability-and-extendability","title":"Portability and extendability","text":"<p>A facade pattern is introduced in OSs with the integration of system calls. This is the way for processes to ask for a privileged service (like reads or writes in buffers) that are taken care by the kernel. Another pattern used by devices is the bridge pattern, which states that the abstraction and the implementation of features must be defined independently.</p>"},{"location":"kernel/overview/#architectures","title":"Architectures","text":"<p>The design of an OS can follow different architectural approaches.</p> <ul> <li>A bare metal (no OS) option is chosen when there's only a single running app, which requires low latency and low power consumption.</li> <li>A monolithic OS uses a single large binary for the kernel, which itself contains all required drivers (Linux).</li> <li>There is also the option to have a monolithic OS with modules, which allows external components to be linked at runtime.</li> <li>A micro kernel instead avoids all non-essential components, which need to be included in user space.</li> <li>An hybrid OS resembles micro kernels, but includes some other services to increase performance (Windows and MacOS).</li> <li>Finally, a library OS comes in the form of libraries, which must be selected and compiled with the corresponding configuration code.</li> </ul>"},{"location":"kernel/processes/","title":"Processes","text":""},{"location":"kernel/processes/#tasks","title":"Tasks","text":"<p>In Linux, both processes and threads fall into the definition of task.</p> <p>A task consists of</p> <ul> <li>a task counter,</li> <li>a stack in user mode,</li> <li>a stack in kernel mode,</li> <li>a set of CPU registers,</li> <li>an address space.</li> </ul> <p>If the address space is shared with another task, those are called threads, processes otherwise. Kernel threads share the whole kernel space.</p> <p>The PCBs of tasks are represented by the <code>task_struct</code>, organized in a list. The pointer to the current running process' node may be stored either in a register or in the kernel stack. This struct contains various data; the most important ones are</p> <ul> <li><code>preempt_count</code>, that allows context switches to happen only in safe points,</li> <li><code>thread_struct</code>, which contains values of registers of non-running tasks,</li> <li><code>mm_struct</code>, storing memory mappings.</li> </ul>"},{"location":"kernel/processes/#states-and-queues","title":"States and queues","text":"<p>Tasks can fall into different states.</p> <ul> <li>Running: task is executing in the CPU</li> <li>Ready: task is in queue ready for being executed</li> <li>Stopped: task is paused, waiting for a signal to continue.</li> <li>Interruptible sleep: task is waiting for a resource, but can receive signals.</li> <li>Uninterruptible sleep: task is waiting for the end of an atomic action, ignoring signals.</li> </ul> <p>When <code>wait_event</code> or <code>wait_event_interruptible</code> is called, the process is put into the queue of the corresponding waited event as a <code>wait_queue_entry</code>. When the event occurs, a <code>wake_up</code> function is called in order on each process in the queue, marking processes as running. However, not all processes are woken up: processes marked with and \"exclusive\" flag (which are stored in a separate queue) need exclusive access to the resource; therefore, only the first is woken up, skipping the others. All non-exclusive processes gets awoken.</p>"},{"location":"kernel/processes/#kernel-initialization","title":"Kernel initialization","text":"<p>In order to create a new process, the <code>fork</code> function (invoking <code>sys_clone</code>) is called. The process is duplicated, giving the child a new PID and PPID (Parent PID). The copy differs only for some resources, such as pending signals. Memory space is not duplicated, as it remains shared until one process writes on a page, which gets thus copied (Copy on Write).</p> <p>On startup, the kernel executes a series of initialisation calls in a well defined trace.</p> <ol> <li><code>start_kernel</code>, found in <code>/init/main.c</code>, executes architecture setup routines</li> <li><code>rest_init</code> creates a new kernel thread executing <code>kernel_init</code></li> <li><code>rest_init</code> then evolves into the low-power, PID 0 idle thread</li> <li><code>kernel_init</code> then calls <code>initrd_load</code>, responsible for initial RAM setup</li> <li><code>kernel_execve</code> runs <code>/bin/init</code> with PID 1, initialising long-running services</li> </ol> <p>The init task have been implemented in many ways. Initially, it was managed by the SystemV process, which has now been replaced by a more modern SystemD. SystemV worked by dividing startup scripts into levels, for then executing each one by one (which is slow). SystemD is more simple, efficient and prone to parallelisation: each service is defined in its corresponding text file containing all necessary configurations and dependencies; services can be grouped in targets, and a tool is provided for managing (start, stop, query) services. SystemD is declarative: you define what you need and the system figures out how to achieve it.</p>"},{"location":"kernel/processes/#task-scheduling","title":"Task scheduling","text":"<p>During the execution of instructions, exceptions may arise. These differ from interrupts as they are internal and synchronous, and are to be managed before preemption (context switch). Interrupts on the other hand are external and asynchronous, as they come from resources, often hardware device. Furthermore, interrupts will pause and resume programs, whilst exceptions are instantly resolved, even if they will make the program crash. Exceptions are found, for instance, upon a division by zero or invalid memory address accesses. When an exception is found, <code>preempt_count</code> is increase, and decrease when treated. The <code>switch_to</code> function can be called only when this count gets to zero.</p> <p>Developer can implement their own scheduling policy by calling functions of a scheduling class. Most important ones are:</p> <ul> <li><code>task_tick</code> updates the current task time statistics;</li> <li><code>pick_next_task</code> selects the next from the queue;</li> <li><code>select_task_rq</code> selects the core on which task gets enqueued;</li> <li><code>enqueue_task</code> enqueues the task.</li> </ul> <p>All processes have a priority value. For real-time processes, the <code>rt_priority</code> is a value between 0 and 99. For non-real-time processes, <code>static_prio</code> takes value between 100 and 139, but it is stored as an interval from -20 to +19.</p>"},{"location":"kernel/processes/#completely-fair-scheduling","title":"Completely fair scheduling","text":"<p>CFS assigns time slices based on system load. Each process has a variable <code>vruntime</code> which represents its absolute measure of dynamic priority. The lower it is, the higher is the priority. All processes have consumed their time share if they all have reached the same value for <code>vruntime</code>. The computation of this variable mainly depends on an assigned process weight value (relative to all other's). The higher the weight, the longer is the time slice.</p>"},{"location":"kernel/processes/#earliest-eligible-virtual-deadline-first","title":"Earliest eligible virtual deadline first","text":"<p>EEVDF is similar to CFS but with fixed time slices. Each process tracks its expected CPU time and actual CPU time; their difference is called lag. Processes with positive lag are eligible to run. Among eligible processes, EEVDF selects the one with the earliest virtual deadline to run next.</p> <p>Tasks with higher priority are assigned smaller fixed time slices. Tasks with lower nice values have higher weights and thus higher priority. Moreover, time slices are also influenced by the process latency-nice value, which is introduced to prioritize processes which need short time slices but are latency-sensitive.</p>"},{"location":"kernel/processes/#additional-fairness","title":"Additional fairness","text":"<p>If a user has more threads than another, it would get more CPU time as it would have more tasks in the run queue. To ensure that a user with fewer threads isn't penalized, the scheduler is modified to account for user\u2011level shares first. This is achieved by looking at users as they were single tasks in a root run queue; then, each user has assigned a specific run queue where sub-tasks take turn to execute. Task groups are called CGroups. Furthermore, CGroups allow to move tasks in two groups: workload and background. Weights can be assigned to both to allocate more resources to workload ones.</p> <p>Multi-CPU systems also allow for a better load balancing. The designated core is the idlest core which periodically tries to steal threads from busiest ones, to balance the load. This process relies on a cache hierarchy tree, representing cores grouped by shared cache levels. Each core maintains a load value, and cores propagate load information upward in the hierarchy so that nearby cores are aware of which have higher loads.</p>"},{"location":"memory/physical-address-space/","title":"Physical address space","text":""},{"location":"memory/physical-address-space/#buddy-allocator","title":"Buddy allocator","text":"<p>Physical memory is allocated big chunks at a time; to efficiently allocate many smaller and different-size objects, the memory block gets partitioned with the buddy algorithm.</p> <p>First, round up the request to the next power-of-two size. Check the free list for that size; if available, pop and use one block. If empty, take a block from the next larger free list, recursively split into buddies, push the unused buddies to their sizes list, and allocate the one you need.</p> <p>If no space is available, allocate pages to create a new buddy block. When releasing a block, check if its buddy is also free, and in that case, merge them into a one big free block, possibly going up recursively.</p> <p><code>kmalloc</code> is built on top of the slab allocator, itself sitting on top of the buddy allocator. <code>vmalloc</code> lives directly on top of the buddy allocator.</p>"},{"location":"memory/physical-address-space/#zonal-page-allocation","title":"Zonal page allocation","text":"<p>System's memory is partitioned among each NUMA node. Each node has an instance in the <code>pgdata_list</code>, with which it manages its own chunk of memory.</p> <p>Each node's memory is partitioned in \"zones\", each being a physical memory range addressed to specific use cases. Each zone manages its free pages, storing page descriptors in a free list.</p> <p>Essential zones are the <code>ZONE_NORMAL</code> (upper addresses) and <code>ZONE_DMA</code> (lower addresses): the first is mapped by the kernel while the second is used by devices.</p>"},{"location":"memory/physical-address-space/#user-space-page-caching","title":"User space page caching","text":"<p>Physically allocated pages can be anonymous (swap) or linked to files. Each page is cached in a <code>struct page</code> descriptor.</p> <p>Two types of mapping exist.</p>"},{"location":"memory/physical-address-space/#forward-mapping","title":"Forward mapping","text":"<p>Forward mapping is used to find the <code>struct page</code> given a virtual address.</p> <p>For example, given a file <code>struct inode</code>, it contains a <code>struct address_space</code>, which stores mappings between file's offsets and corresponding physical pages.</p> <p>Anonymous pages (pages not linked to files) are linked to the swapfile; when they can be swapped out, they enter the swap cache. The swap cache is implemented using a single global <code>struct address_space</code> named <code>swapper_space</code>, indexed by swap location rather than file offset.</p> <p>Given a <code>struct file</code> or a swap location it is therefore possible to get the corresponding underlying physical <code>struct page</code>. This is useful for example to check residency: the <code>struct page</code> can tell if its data is stored in RAM, if it's up to date or if it's to be reloaded from disk.</p>"},{"location":"memory/physical-address-space/#reverse-mapping","title":"Reverse mapping","text":"<p>Reverse mapping from a <code>struct page</code> identifies VMAs mapping that physical page. This is necessary when evicting the page, as all referencing virtual pages must be invalidated in the PTEs (Page Table Entry) of processes. This struct also keeps flags representing the dirty state of the page.</p> <p>The <code>struct page</code> contains a <code>mapping</code> pointer referencing either a file's <code>address_space</code> or an anonymous memory range (<code>anon_vma</code>) of a process using the page. Via these struct it's possible to reach a list of VMAs using the page. By walking these VMAs, each is checked if containing virtual pages pointing to the evicted page, possibly removing it from PTEs.</p>"},{"location":"memory/physical-address-space/#pfra","title":"PFRA","text":"<p>The Page Frame Reclaim Algorithm is used to unload pages when free space shrinks. It is mainly operated by the <code>kswapd</code> daemon, which runs periodically, but under high loads the buddy allocator can run it itself. Pages are reclaimed via reverse mapping.</p> <p>The reclaim policy is derived from the LRU-like \"clock algorithm\". A circular list keeps pointers to all pages, each containing a R bit representing recent references. When a program uses a page, it sets R=1. When in need of reclaiming a page, the page under the hand (index) is evicted if R=0, otherwise, if R=1, R is set to 0 and the hand moves forward until a reclaimable page is found.</p> <p>File pages are majorly clean at the time of eviction, whilst anonymous ones are always dirty. For this reason, Linux keeps two different groups based on page type, to better balance use cases.</p> <p>Each group (file's and anonymous pages) applies the clock algorithm on two different circular list: actives and inactives. When a file is accessed, it is put in inactives with R=1. When <code>shrink_list</code> is called, it runs the clock algorithm on inactives. When <code>refill_inactive_zone</code> is called, it runs the algorithm on actives, evicting pages to inactives. <code>mark_page_accessed</code> sets R=1 on the page; when it gets called on a R=1 inactive page, that gets moved to actives, setting R=0.</p> <p>The inactive list should to be kept balanced with the active one, with its size matching the working set (number of \"recently\" used pages). The PFRA algorithm can thus be called periodically reallocating actives to inactives. This is needed as a small inactive list can lead to thrashing, resulting in premature evictions and frequent reloads.</p>"},{"location":"memory/virtual-address-space/","title":"Virtual address space","text":"<p>Linux memory management adopts virtual address spaces, meaning that allocated pages are identified by addresses that do not correspond (but are mapped) to the underlying physical ones.</p> <p>There is a separation between kernel and process memory space. Each process is assigned with a set of numbered pages, mapped both to the process and to the physical address space.</p>"},{"location":"memory/virtual-address-space/#kernel-space","title":"Kernel space","text":"<p>The kernel space is divided in two parts: the logical and the virtual kernel address spaces. Logical pages are allocated with <code>kmalloc</code> and map one-to-one with physical ones (just offset) as they reside in the first reserved RAM addresses. Contiguously allocated pages will be contiguous both virtually and physically. The virtual space (allocated with <code>vmalloc</code>) instead does not map directly to physical addresses.</p>"},{"location":"memory/virtual-address-space/#kmalloc","title":"<code>kmalloc</code>","text":"<pre><code>void *kmalloc(size_t size, gfp_t flags)\n</code></pre> <p><code>kmalloc</code> is used for small allocations of continuous physical memory. Properties of the memory location and allocation process change based on flags.</p> <ul> <li><code>kmalloc</code> + <code>GFP_KERNEL</code>: default for small kernel objects (structs, lists),     space is physically contiguous and provides fast access. It may sleep.</li> <li><code>kmalloc</code> + <code>GFP_USER</code>: used to create user-space buffers with non-movable     pages (memory shared by two processes, when DMA accesses are required).     It may sleep.</li> <li><code>kmalloc</code> + <code>GFP_ATOMIC</code>: used to allocate memory from interrupt handlers.     It never sleeps</li> </ul>"},{"location":"memory/virtual-address-space/#get-free-pages","title":"Get free pages","text":"<pre><code>unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order)\n</code></pre> <p><code>kmalloc</code> has some overhead and may lack controllability on some specific actions. <code>__get_free_pages</code> returns large chunks of contiguous pages free of built-in protection mechanisms, giving the highest of control and performance but requiring manual management, which if done wrong may lead to system degradation.</p>"},{"location":"memory/virtual-address-space/#allocate-pages","title":"Allocate pages","text":"<pre><code>struct page *alloc_pages_node(int nid, gfp_t gfp_mask, unsigned int order)\n</code></pre> <p>In NUMA architectures (Non-Uniform Memory Access) pieces of memory can be tight to the CPU they are closer to, to speedup accesses. Other processors can access others' memory but access time increases; for this reason, pages can be allocated providing the specific NUMA node (core) they get assigned to.</p>"},{"location":"memory/virtual-address-space/#vmalloc","title":"<code>vmalloc</code>","text":"<pre><code>void *vmalloc(unsigned long size)\n</code></pre> <p><code>vmalloc</code> allocates a large virtually contiguous kernel heap with scattered physical pages. It is slower than other allocation methods as it needs to setup and modify memory table mapping pages. Furthermore, it may sleep, thus cannot be used in atomic contexts.</p>"},{"location":"memory/virtual-address-space/#slab-allocator","title":"Slab allocator","text":"<pre><code>void *kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags)\n</code></pre> <p>Page\u2011based allocation is not suitable when dealing with objects. To handle fixed\u2011size data structures efficiently the kernel uses slabs.</p> <p>A slab is an allocated space dedicated to store structs of a specific size. The space allocated for the slab is subdivided in consecutive blocks of that specific object size. Each slab maintains a free-list, marking each internal block as used or not, based on requested object allocations and frees.</p> <p>As said, a slab is initialized at a given size, which may be multiple pages long, but when it fills up, another can be instantiated. Therefore, when allocating an object multiple slabs may be available for that object size. A struct named <code>kmem_cache</code> is used to list full, partial and empty slabs for their specific object they map. Multiple cache instances are created, one for each distinct object size. Moreover, each NUMA node manages its own <code>kmem_cache</code> structs.</p> <p>The slab allocator provides two main classes of caches: dedicated caches are used to map commonly used kernel types with their optimized size; generic caches are instead general purpose and provide slots of set sizes (usually powers of two).</p>"},{"location":"virtualisation/containers/","title":"Containers","text":"<p>Containers are a way to isolate processes and trick them to thinks they are the only ones running on the machine. Containers are not virtual machines: processes running in containers are normal processes running on the host kernel (there is not guest kernel, kernel is shared with the host). Therefore, there is no performance drop.</p>"},{"location":"virtualisation/containers/#namespaces","title":"Namespaces","text":"<p>Linux implements PID namespaces: when a process creates a new namespace, it becomes the root of its tree and all sub-processes receive two PIDs (old and new one). A child namespace cannot interact with the parent process tree. Exploiting this technology, container applications use their own independent process trees.</p> <p>There are many types of namespaces, below some examples.</p> <ol> <li>Unix Timeshared System (UTS) gives each process tree a host and a domain     name.</li> <li>Mount namespaces allow different views of the file system.</li> <li>Network namespaces offer separate network stacks to processes.</li> <li>User namespaces let processes inside containers to have different user and     group identifiers.</li> </ol>"},{"location":"virtualisation/hardware-based/","title":"Hardware-based","text":"<p>Hardware-based virtualisation solves many software-based problems by introducing different execution modes. Ring aliasing is removed, with the guest supervisor working as a normal supervisor, letting the VMM decide which privileged instructions to trap (truly sensitive) and which to let pass without VMM intervention; this also improves performance. Then, the hardware offers a dedicated space to store guest context, ditching most shadow data structures.</p> <p>HW virtualisation is implemented in different ways depending on the ISA. x86 offers a VM control block to store the whole guest context, swtiching with a dedicated instruction. On other systems like ARM's, special registers hold just minimal VM information.</p>"},{"location":"virtualisation/hardware-based/#intel-vt-x","title":"Intel VT-x","text":"<p>With Intel VT-x design it is possible to create a dedicated non-root mode, which duplicates the entire CPU state. Root mode is where the hypervisor operates from, while VMs live in this non-root mode. The switch between the two can be done anytime. Executions in non-root mode can be recursively virtualised too.</p> <p>Each mode uses its own interrupt flags, so that external interrupts, triggering in root mode, can instigate an easy transition to non-root mode. Regarding privileged instruction operations to data structures, those happen in a vCPU represented in the VMCS (Virtual Machine Control Structure), skipping trapping, being auto-managed.</p> <p>Intel specifically introduced a set of new instructions for managing virtualisation. Here are some of them.</p> <ul> <li><code>vmxon</code> and <code>vmxoff</code> turn VT-x on and off.</li> <li><code>vmlaunch</code> enters non-root mode directly.</li> <li><code>vmresume</code> enters non-root mode but loading the guest state in the VMCS.</li> <li><code>vmexit</code> returns to root mode, saving state.</li> </ul> <p>VT-x also enhances paging, introducing an Extended Page Table (EPT). Instead of relying on a single virtual-to-physical page mapping register (<code>ec3</code>), it offers two, one for the guest to map guest-virtual to host-virtual, and one for typical host-virtual to host-physical pages. This eliminates exits as the EPT can be directly managed by the guest without requiring VMM intervention.</p>"},{"location":"virtualisation/hardware-based/#kvm","title":"KVM","text":"<p>Kernel-based Virtual Machine is a virtualisation module directly built into the Linux kernel, letting it act as a type 1 hypervisor, having VMs run as threads. It's efficient as reuses Linux's scheduler, memory management, and drivers support. It can be started by opening the virtual <code>/dev/kvm</code> device. Typically, you'd place VM executions in a loop where the kernel takes control back when guest exits for whichever reasons.</p> <pre><code>// Enter hardware virtualisation.\nint kvm_fd = open(\"/dev/kvm\");\n// Create guest memory.\nint vm_fd = ioctl(kvm_fd, KVM_CREATE_VM, 0);\n// Allocate vCPU.\nint vcpu_fd = ioctl(vm_fd, KVM_CREATE_VCPU, 0);\n\n// Map VMCS KVM abstraction from vCPU.\nstruct kvm_run *kvm_run = mmap(/* ... */, vcpu_fd, 0);\n\nfor (;;) {\n    // Run vCPU until exit.\n    ioctl(vcpu_fd, KVM_RUN, 0);\n    switch(kvm_run-&gt;exit_reason) {\n        // Emulate IO in user space.\n        case KVM_EXIT_IO: /* ... */\n        // Check interrupt.\n        case KVM_EXIT_HLT: /* ... */\n    }\n}\n</code></pre>"},{"location":"virtualisation/hardware-based/#overcommitment","title":"Overcommitment","text":"<p>VMs do not use 100% of their available memory, therefore, it is possible to allocate more VM memory that physically available. KVM VMs are based on Linux virtual memory, which allows each process to consume all its address space, enabling overcommitment by default.</p>"},{"location":"virtualisation/hardware-based/#ballooning","title":"Ballooning","text":"<p>Ballooning is a way to enforce that VMs use of their physical memory stays under a set threshold. It is accomplished by adopting a special driver, which forces swapping to occur when a VM wants to allocate more memory than physically allowed. This can also be used when needing to free up space for allocating a new VM.</p>"},{"location":"virtualisation/hardware-based/#same-page-merging","title":"Same-page merging","text":"<p>Kernel Same-page Merging (KSM) is a KVM tool that can be enabled and which allows virtual pages with same content to share the same physical page. Comparing pages can be very resource consuming, but it can be beneficial when using applications that produce many instances of identical data. When finding two duplicates, both get marked as CoW and one is redirected to the physical of the other.</p>"},{"location":"virtualisation/hardware-based/#qemu","title":"Qemu","text":"<p>Qemu is a program which can create and manage VMs and emulate devices. It can also do pure CPU emulation, but with KVM enabled it can unload CPU execution to KVM directly for near-native speed. A big challenge for VMMs is managing hardware interactions/pass-through exposed to VMs. Qemu offers an ecosystem of emulated devices accessible via the VirtIO interface. VirtIO implements a standard set of device interfaces including network adapters, disk drives and IO controllers.</p> <p>Through a shared memory region (<code>virtqueue</code>), the VirtIO guest driver communicates directly with the hypervisor, knowing it is in a virtual machine; this concept is called paravirtualisation. Unlike full virtualisation, paravirtualisation modifies the guest OS to efficiently interface the hypervisor through hypercalls, reducing traps and emulation overhead.</p>"},{"location":"virtualisation/hardware-based/#xen","title":"Xen","text":"<p>Xen is a type 1 hypervisor running on bare metal. Critical (privileged) instructions requiring ring 0 get replaced by hypercalls. In this way, user space applications typically remain unaffected, but this design requires guest OS (DomU) instructions in ring-0 to be ported to ring-1. Xen also support the use of Dom0, which is a specially privileged domain running Linux completely natively. DomU drivers can communicate with Dom0 directly, just like VirtIO.</p>"},{"location":"virtualisation/introduction/","title":"Introduction","text":"<p>There are two types of virtual machines:</p> <ul> <li>process VMs are designed to provide a platform-independent environment to     applications;</li> <li>system VMs provide an environment to multiple operating systems.</li> </ul> <p>A system VM is defined as an isolated duplicate of the real system, managed by a VMM (Virtual Machine Monitor) or hypervisor which translate accesses to physical resources.</p> <p>There are two types of hypervisors.</p> <ul> <li>Type 1: it runs native on bare metal.</li> <li>Type 2: it runs within another OS.</li> </ul> <p>System virtualisation must meet three key requirements.</p> <ul> <li>Fidelity: the behaviour of the VM should match the real machine.</li> <li>Safety: VMs cannot override VMM control.</li> <li>Efficiency: only minor speed decreases are tolerated.</li> </ul>"},{"location":"virtualisation/introduction/#benefits","title":"Benefits","text":"<p>Implementing VMs is useful for a variety of reasons. Multiple VMs can be packed together on a single physical machine, so that its resources get fully exploited minimizing idles (consolidation). This also helps with management and administration costs, by allowing the usage of fewer bigger machines, enabling horizontal scalability and better responding to variable workloads. Infrastructure gets also standardized, simplifying software distribution and isolating networks and storage. Finally, they help with fault tolerance and sandbox security tests.</p>"},{"location":"virtualisation/introduction/#instruction-sensitivity","title":"Instruction sensitivity","text":"<p>Instructions can be privileged or unprivileged. The core principle of virtualisation is to run guest privileged instructions as unprivileged. An instruction is virtualisation sensitive if is at least one of the following.</p> <ul> <li>Control sensitive, meaning it directly modifies system state.</li> <li>Behavior sensitive, meaning it acts differently in user and supervisor mode.</li> </ul> <p>If sensitive instructions fall in the set of privileged instructions, a VMM can be built (sufficient condition).</p>"},{"location":"virtualisation/introduction/#sw-and-hw-virtualisation","title":"SW and HW virtualisation","text":"<p>Virtualisation can be achieved on two levels: hardware and software.</p> <p>Software-based virtualisation adopts a process called deprivileging. The hypervisor intercepts (traps) and emulates all privileged instructions coming from guests.</p> <p>Hardware-based virtualisation exploits CPU virtualisation built-in support, limiting the host intervention. All processors typically come with control blocks on which to store guest state to be resumed later explicitly by the VMM. Furthermore, fewer specific events require switching to the hypervisor context.</p>"},{"location":"virtualisation/software-based/","title":"Software-based","text":"<p>Software-based virtualisation adopts a process called deprivileging, for which guest's supervisor instructions are translated into host user space. The VMM further intercepts all guest modifications to kernel data structures, such as the IDT (Interrupt Descriptor Table), redirecting them to internally managed replicates, when necessary. Another example of these \"shadow tables\" are page tables: the hypervisor catches the guest OS when attempting to set its own page table, and it builds a corresponding shadow table where guest \"physical\" pages are translated to host virtual ones.</p>"},{"location":"virtualisation/software-based/#problems","title":"Problems","text":"<p>Some architectures originally couldn't be efficiently virtualised because not all \"sensitive\" instructions were privileged.</p> <p>A guest supervisor could detect deprivileging by reading some hardware registers seeing mismatched CPL (Current Privilege Level) or host tables, exposing the VMM presence. Furthermore, constant emulation (<code>sysenter</code>/<code>sysexit</code> called for every guest <code>syscall</code>) required too many traps, making software-only VMMs impractical.</p>"}]}